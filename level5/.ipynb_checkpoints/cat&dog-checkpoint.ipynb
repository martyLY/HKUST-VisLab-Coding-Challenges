{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 5\n",
    "-------------------\n",
    "In this task, I will do the famous Dogs vs. Cats competition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Data Preprocessing\n",
    "### Image Processing\n",
    "Note that the data given are of various sizes. Need to standardize image specifications. The network input of VGG is uniform 3×224×224, so the image is processed to the corresponding size. Use ``torchvision.transforms`` to resize the image and convert it to``torch.tensor``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM = transforms.Compose([transforms.Resize((256,256)),\n",
    "                       transforms.RandomCrop((224,224)),\n",
    "                       transforms.ToTensor(),\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By randomly cropping the training set pictures, you can introduce more randomness. On the one hand, you can increase the training data (the pictures are different for different epochs), and on the other hand, reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very important operation to prevent overfitting is image normalization. Image normalization is not simply every position / 256.0, but requires more processing. PyTorch has implemented a normalization function. The parameters that need to be controlled are the variance and the mean. Standing on the shoulders of giants, the commonly used parameters in image recognition are (normalize each pixel to [−1,1]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalize(mean = [0.485, 0.456, 0.406], \n",
    "          std = [0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the training set images are preprocessed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM = transforms.Compose([transforms.Resize((256,256)),\n",
    "                                transforms.RandomCrop((224,224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set does not need to be randomly cropped, just use the original image to scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "PyTorch's built-in ```Dataset``` can automatically implement data loading. First, you need to define an inheritance of ```torch.utils.data.Dataset```. There are two ways to achieve this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One is the **Dynamic Loading** type: only the file path of each picture is stored in the memory. When the picture needs to be called later, the picture is read on-site, the single picture is pre-processed and returned. So no pre-processing time is required to run directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disadvantage is that in the case of many epochs, the total time cost of repeatedly reading the picture will be large. Because the training process is based on batch as the smallest unit, the memory overhead is completely controlled by batch_size except for storage network parameters, which can be automatically adjusted according to memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ _ ^ Poor lower middle peasant options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(data.Dataset):\n",
    "    def __init__(self, image_list, label_list):\n",
    "        self.data = image_list\n",
    "        self.label = label_list\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        global TRANSFORM\n",
    "        img = Image.open(self.data[index])\n",
    "        data = TRANSFORM(img)\n",
    "        img.close()\n",
    "        return data.cuda(),torch.cuda.FloatTensor([self.label[index]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other is the **Preload**: the entire picture library is preprocessed and loaded into memory, and the pictures can be returned each time in the future. It takes about 40s to read the picture library at the beginning of the experiment, which is acceptable. After that, the speed of calling pictures will be very fast, and there is a significant improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the memory overhead is particularly huge. It seems that using numpy.ndarray storage also requires about 20GB of memory (no specific number is measured). If you use ``torch.tensor``, it will explode further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ _ ^ Big Capitalist Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(data.Dataset):\n",
    "    def __init__(self, image_list, label_list):\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        for i in range(len(image_list)):\n",
    "            img = Image.open(image_list[i])\n",
    "            self.data.append(TRANSFORM(img))\n",
    "            img.close()\n",
    "            self.label.append(label_list[i])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index].cuda(),torch.cuda.FloatTensor([self.label[index]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. Here, a unified random seed is used to ensure that the verification set and training set divided at each run are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    np.random.seed(998244353)\n",
    "    torch.manual_seed(998244353)\n",
    "    image_list = []\n",
    "    label_list = []\n",
    "    for i in range(ORIGIN_DATA_SIZE):\n",
    "        image_list.append(INPUT_PATH+\"/train/cat.{0}.jpg\".format(i))\n",
    "        label_list.append(0)\n",
    "        image_list.append(INPUT_PATH+\"/train/dog.{0}.jpg\".format(i))\n",
    "        label_list.append(1)\n",
    "    n = int(ORIGIN_DATA_SIZE*2*RATIO)\n",
    "    train_data = ImageDataset(image_list[:n],label_list[:n])\n",
    "    validate_data = ImageDataset(image_list[n:],label_list[n:])\n",
    "    image_list = []\n",
    "    for i in range(TARGET_DATA_SIZE):\n",
    "        image_list.append(INPUT_PATH+\"/test/{0}.jpg\".format(i+1))\n",
    "    test_data = ImageDataset(image_list,[0]*TARGET_DATA_SIZE)\n",
    "    np.random.seed()\n",
    "    torch.seed()\n",
    "    return train_data,validate_data,test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 PyTorch Implements VGG\n",
    "### VGG Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG model's reference [VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION](https://arxiv.org/pdf/1409.1556.pdf \"ref\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a general VGG will be implemented according to the VGG version structure table. It is mainly achieved by using ``torch.nn.Sequential ()`` of PyTorch and ``add_module ()`` which can dynamically add connection layers. Here is the basic structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, name=\"11\"):\n",
    "        super(VGG, self).__init__()\n",
    "        self.name = \"VGG\"+name\n",
    "        self.conv = nn.Sequential()\n",
    "        i = 1; p = 1\n",
    "\n",
    "        # ... Different versions of VGG\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*7*7,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096,1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000,1),\n",
    "            nn.Sigmoid()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fully connected part of all VGG versions is certain, only the part omitted in the middle is different. When you create a VGG, you need to define a name, and select from``[\" 11 \",\" 11-LRN \",\" 13 \",\" 16-1 \",\" 16 \",\" 19 \"]``.Names that are default or not in the structure table are considered VGG11. Because `` add_module () ``needs a name to create a layer, `` i ``and `` p ``are used to identify the labels of different convolution layers and different pooling layers, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG Implementation\n",
    "Create a convolutional layer using the following method, and you can see that each parameter has been explicitly noted. Note that the number of channels between different convolutional layers must match. In order to ensure that the size of the picture does not change, each 3 × 3 convolution uses ``padding = 1`` and each 1 × 1 convolution uses ```padding = 0```. Therefore, the 3 × 224 × 224 picture is directly passed in initially Instead of a 3 × 227 × 227 picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=  3, out_channels= 64, kernel_size=3, stride=1, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following method to create an activation function layer (the standard uses ReLU), and there is an activation function layer behind each convolution layer. The two labels can be the same, so the label ```i``` is updated after each activation of the function layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-70eb55c991cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ReLU-{0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pooling layer using the following method. In VGG, only a pooling layer of the above type and parameter is used: 2 × 2 MaxPooling. Note the update of the pooling layer label ```p```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.conv.add_module('MaxPooling-{0}'.format(p),nn.MaxPool2d(kernel_size=2, stride=2));p+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An LRN layer (Local Response Norm, local response normalization) is created using the following method. The LRN layer is used only once in VGG11-LRN. The main idea is to normalize the signals of adjacent channels in the middle of the neural network. Therefore, when a certain neuron signal is relatively large, the relative signal size of peripheral neurons will decrease, which can mimic the excitement of a neuron. Phenomenon of peripheral neuron suppression. The main function is to prevent overfitting, especially on neural networks using ReLU activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.conv.add_module('LRN',nn.LocalResponseNorm(size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input is ```batch_size × 3 × 224 × 224``` dimensions, and the output is ```batch_size × 512 × 7 × 7```. Next, the convolutional layer with input ```batch_size × 25088``` is connected. A reorganization of the neurons is required, so the forward propagation process is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    x = self.conv(x)\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    x = self.fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start building VGG!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VGG(\"19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 Adjustment Parameter\n",
    "The RMSprop optimizer can automatically adjust parameters. Just call PyTorch's built-in RMSprop optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(net.parameters(), lr=LR, alpha=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 Training\n",
    "Due to the limitation of the storage capacity (save the parameter + optimizer more than 1G, remember to save in the middle stage, save once every UPDATE epoch, and record Validation Error and Validation Loss when saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optimizer):\n",
    "    global net\n",
    "    global EPOCH\n",
    "    global BATCH_SIZE\n",
    "    global train_data\n",
    "    global validate_data\n",
    "    global PROGRAM_START\n",
    "    print('['+net.name+'] with optimizer ['+str(type(optimizer))+']:')\n",
    "    train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    validate_loader = data.DataLoader(validate_data, batch_size=1, shuffle=False, num_workers=0)\n",
    "    BATCH = len(train_loader)\n",
    "    m = len(validate_loader)\n",
    "    for epoch in range(EPOCH):\n",
    "        EPOCH_START = time.time()\n",
    "        print(\"\\tEpoch #{0}/{1}:\".format(epoch+1,EPOCH))\n",
    "        for batch,(x,y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            t = net(x)\n",
    "            loss = LOSS_FUNC(t,y)\n",
    "            print(\"\\t\\tBatch #{0}/{1}: \".format(batch+1,BATCH) + \"Loss = %.6f\"%float(loss))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            L = 0.\n",
    "            E = 0.\n",
    "            for batch,(x,y) in enumerate(validate_loader):\n",
    "                t = net(x)\n",
    "                L += float(LOSS_FUNC(t,y))\n",
    "                E += float((float(t[0][0])>0.5)!=y)\n",
    "            print(\"\\t  Validation Loss = %.6f. Error Rate = %.3f%%\"%(L/m,E*100/m))\n",
    "            if((epoch+1)%UPDATE==0):\n",
    "                torch.save(net.state_dict(),OUTPUT_PATH+\"/{0}[{1}]\".format(net.name,epoch+1)+\"-L(%.6f)E(%.3f).pt\"%(L/m,E*100/m))\n",
    "                torch.save(optimizer.state_dict(),OUTPUT_PATH+\"/{0}[{1}]\".format(net.name,epoch+1)+\"-L(%.6f)E(%.3f)-optimizer.pt\"%(L/m,E*100/m))\n",
    "        print(\"\\t  Finish epoch #{0}\".format(epoch+1)+\" in %.4f s.\"%(time.time()-EPOCH_START)+\" Total Time Cost = %.4f s.\"%(time.time()-PROGRAM_START))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set has 25,000 pictures, of which 99% or 24750 are used as the training set and 250 are used as the test set. Update every 5 epochs.\n",
    "A larger batch_size is better, but without GPU and memory, I can only set it to 75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One technique is to use binary SWITCH to control program operation, 1 for training and 2 for loading ready-trained models. Then if SWITCH = 1, the ready-trained model is trained and then predicted, SWITCH = 2 is loaded into the ready-trained model and predicted, and SWITCH = 3 is loaded into the ready-trained model and predicted after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "INPUT_PATH = \"\"\n",
    "OUTPUT_PATH = \"\"\n",
    "ORIGIN_DATA_SIZE = 12500\n",
    "TARGET_DATA_SIZE = 12500\n",
    "RATIO = 0.99\n",
    "EPOCH = 15\n",
    "BATCH_SIZE = 75\n",
    "LOSS_FUNC = nn.BCELoss()\n",
    "LR = 0.0001\n",
    "SWITCH = 3\n",
    "UPDATE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Configurations\n",
    "INPUT_PATH = \"\"\n",
    "OUTPUT_PATH = \"\"\n",
    "ORIGIN_DATA_SIZE = 12500\n",
    "TARGET_DATA_SIZE = 12500\n",
    "RATIO = 0.99\n",
    "EPOCH = 15\n",
    "BATCH_SIZE = 75\n",
    "LOSS_FUNC = nn.BCELoss()\n",
    "LR = 0.0001\n",
    "SWITCH = 3\n",
    "UPDATE = 5\n",
    "TRANSFORM = transforms.Compose([transforms.Resize((256,256)),\n",
    "                                transforms.RandomCrop((224,224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))\n",
    "                               ])\n",
    "PARAMETERS = \"\"\n",
    "\n",
    "class ImageDataset(data.Dataset):\n",
    "    def __init__(self, image_list, label_list):\n",
    "        self.data = image_list\n",
    "        self.label = label_list\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        global TRANSFORM\n",
    "        img = Image.open(self.data[index])\n",
    "        data = TRANSFORM(img)\n",
    "        img.close()\n",
    "        return data.cuda(),torch.cuda.FloatTensor([self.label[index]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, name=\"11\"):\n",
    "        super(VGG, self).__init__()\n",
    "        self.name = \"VGG\"+name\n",
    "        self.conv = nn.Sequential()\n",
    "        i = 1; p = 1\n",
    "        self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=  3, out_channels= 64, kernel_size=3, stride=1, padding=1))\n",
    "        self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        if name in [\"13\",\"16-1\",\"16\",\"19\"]:\n",
    "            self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels= 64, out_channels= 64, kernel_size=3, stride=1, padding=1))\n",
    "            self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        if name in [\"11-LRN\"]:\n",
    "            self.conv.add_module('LRN',nn.LocalResponseNorm(size=2))\n",
    "        self.conv.add_module('MaxPooling-{0}'.format(p),nn.MaxPool2d(kernel_size=2, stride=2));p+=1   # 224 -> 112\n",
    "\n",
    "        self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels= 64, out_channels=128, kernel_size=3, stride=1, padding=1))\n",
    "        self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        if name in [\"13\",\"16-1\",\"16\",\"19\"]:\n",
    "            self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1))\n",
    "            self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        self.conv.add_module('MaxPooling-{0}'.format(p),nn.MaxPool2d(kernel_size=2, stride=2));p+=1   # 112 -> 56\n",
    "\n",
    "        self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1))\n",
    "        self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1))\n",
    "        self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        if name in [\"16\",\"19\"]:\n",
    "            self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1))\n",
    "            self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        if name in [\"16-1\"]:\n",
    "            self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1, stride=1, padding=0))\n",
    "            self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        if name in [\"19\"]:\n",
    "            self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1))\n",
    "            self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        self.conv.add_module('MaxPooling-{0}'.format(p),nn.MaxPool2d(kernel_size=2, stride=2));p+=1   # 56 -> 28\n",
    "\n",
    "        self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1))\n",
    "        self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1))\n",
    "        self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        if name in [\"16\",\"19\"]:\n",
    "            self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1))\n",
    "            self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        if name in [\"16-1\"]:\n",
    "            self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, stride=1, padding=0))\n",
    "            self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        if name in [\"19\"]:\n",
    "            self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1))\n",
    "            self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        self.conv.add_module('MaxPooling-{0}'.format(p),nn.MaxPool2d(kernel_size=2, stride=2));p+=1   # 28 -> 14\n",
    "\n",
    "        self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1))\n",
    "        self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1))\n",
    "        self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        if name in [\"16\",\"19\"]:\n",
    "            self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1))\n",
    "            self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        if name in [\"16-1\"]:\n",
    "            self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, stride=1, padding=0))\n",
    "            self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        if name in [\"19\"]:\n",
    "            self.conv.add_module('conv-{0}'.format(i),nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1))\n",
    "            self.conv.add_module('ReLU-{0}'.format(i),nn.ReLU());i+=1\n",
    "        self.conv.add_module('MaxPooling-{0}'.format(p),nn.MaxPool2d(kernel_size=2, stride=2));p+=1   # 14 -> 7\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*7*7,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096,1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def train(optimizer):\n",
    "    global net\n",
    "    global EPOCH\n",
    "    global BATCH_SIZE\n",
    "    global train_data\n",
    "    global validate_data\n",
    "    global PROGRAM_START\n",
    "    print('['+net.name+'] with optimizer ['+str(type(optimizer))+']:')\n",
    "    train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    validate_loader = data.DataLoader(validate_data, batch_size=1, shuffle=False, num_workers=0)\n",
    "    BATCH = len(train_loader)\n",
    "    m = len(validate_loader)\n",
    "    for epoch in range(EPOCH):\n",
    "        EPOCH_START = time.time()\n",
    "        print(\"\\tEpoch #{0}/{1}:\".format(epoch+1,EPOCH))\n",
    "        for batch,(x,y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            t = net(x)\n",
    "            loss = LOSS_FUNC(t,y)\n",
    "            print(\"\\t\\tBatch #{0}/{1}: \".format(batch+1,BATCH) + \"Loss = %.6f\"%float(loss))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            L = 0.\n",
    "            E = 0.\n",
    "            for batch,(x,y) in enumerate(validate_loader):\n",
    "                t = net(x)\n",
    "                L += float(LOSS_FUNC(t,y))\n",
    "                E += float((float(t[0][0])>0.5)!=y)\n",
    "            print(\"\\t  Validation Loss = %.6f. Error Rate = %.3f%%\"%(L/m,E*100/m))\n",
    "            if((epoch+1)%UPDATE==0):\n",
    "                torch.save(net.state_dict(),OUTPUT_PATH+\"/{0}[{1}]\".format(net.name,epoch+1)+\"-L(%.6f)E(%.3f).pt\"%(L/m,E*100/m))\n",
    "                torch.save(optimizer.state_dict(),OUTPUT_PATH+\"/{0}[{1}]\".format(net.name,epoch+1)+\"-L(%.6f)E(%.3f)-optimizer.pt\"%(L/m,E*100/m))\n",
    "        print(\"\\t  Finish epoch #{0}\".format(epoch+1)+\" in %.4f s.\"%(time.time()-EPOCH_START)+\" Total Time Cost = %.4f s.\"%(time.time()-PROGRAM_START))\n",
    "\n",
    "def run(filename):\n",
    "    global net\n",
    "    global test_data\n",
    "    prediction = []\n",
    "    test_loader = data.DataLoader(test_data, batch_size=1, shuffle=False, num_workers=0)\n",
    "    with torch.no_grad():\n",
    "        for i,(x,y) in enumerate(test_loader):\n",
    "            t = net(x)\n",
    "            prediction.append([i+1,float(t[0][0])])\n",
    "    submission = pd.DataFrame(prediction)\n",
    "    submission.columns = ['id','label']\n",
    "    submission.to_csv(filename+\".csv\",index=0)\n",
    "\n",
    "def load():\n",
    "    np.random.seed(998244353)\n",
    "    torch.manual_seed(998244353)\n",
    "    image_list = []\n",
    "    label_list = []\n",
    "    for i in range(ORIGIN_DATA_SIZE):\n",
    "        image_list.append(INPUT_PATH+\"/train/cat.{0}.jpg\".format(i))\n",
    "        label_list.append(0)\n",
    "        image_list.append(INPUT_PATH+\"/train/dog.{0}.jpg\".format(i))\n",
    "        label_list.append(1)\n",
    "    n = int(ORIGIN_DATA_SIZE*2*RATIO)\n",
    "    train_data = ImageDataset(image_list[:n],label_list[:n])\n",
    "    validate_data = ImageDataset(image_list[n:],label_list[n:])\n",
    "    image_list = []\n",
    "    for i in range(TARGET_DATA_SIZE):\n",
    "        image_list.append(INPUT_PATH+\"/test/{0}.jpg\".format(i+1))\n",
    "    test_data = ImageDataset(image_list,[0]*TARGET_DATA_SIZE)\n",
    "    # np.random.seed()\n",
    "    # torch.seed()\n",
    "    return train_data,validate_data,test_data\n",
    "\n",
    "print(\"*****Start\")\n",
    "PROGRAM_START = time.time()\n",
    "train_data,validate_data,test_data = load()\n",
    "print(\"Finish reading data in %.4f s.\"%(time.time()-PROGRAM_START))\n",
    "net = VGG(\"19\")\n",
    "if SWITCH//2==1 and PARAMETERS!=\"\":\n",
    "    net.load_state_dict(torch.load(PARAMETERS+\".pt\"))\n",
    "    print(\"Load Model [\"+PARAMETERS+\".pt] Success!\")\n",
    "net.cuda()\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=LR, alpha=0.9)\n",
    "if SWITCH//2==1 and PARAMETERS!=\"\":\n",
    "    optimizer.load_state_dict(torch.load(PARAMETERS+\"-optimizer.pt\"))\n",
    "    print(\"Load Optimizer [\"+PARAMETERS+\"-optimizer.pt] Success!\")\n",
    "LOSS_FUNC.cuda()\n",
    "if SWITCH% 2==1:\n",
    "    train(optimizer)\n",
    "TRANSFORM = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))\n",
    "                               ])\n",
    "run(OUTPUT_PATH+\"/{0}\".format(net.name))\n",
    "print(\"*****Finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7 Training Results\n",
    "![level5-1](level5-1.png)\n",
    "![level5-2](level5-2.png)\n",
    "There is no GPU avelable, and the memory is not enough, so at the beginning, only 2,000 training samples were selected, so there was a significant overfitting phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest feature of the training curve is overfitting. The accuracy on the training set increases linearly, approaching 100%, while the accuracy on the validation set is between 70% and 72%. Similarly, the loss on the training set decreases linearly to 0, while the loss on the validation set tends to increase after 5 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later I thought of using a pre-trained network and slowly data augmentation to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first time I used jupyter notebook to write deep learning code, the more I wrote, the more it like Readme. Please understand. If you don't understand the notebook, you can move to ``model.py``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pending upgrade. . .**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
